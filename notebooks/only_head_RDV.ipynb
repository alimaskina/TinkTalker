{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Mh1zpSs0157dvalSuRoo00H0WazjWUY-","timestamp":1693468387255},{"file_id":"1jGi8LTMdUKTEF7QDJhC3OwV8EsWNUs-3","timestamp":1693406214336}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install transformers"],"metadata":{"id":"lehF4wrQ9E41","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693472193808,"user_tz":-180,"elapsed":18319,"user":{"displayName":"Iklmn Eprst","userId":"08165371004552365774"}},"outputId":"a421efa3-44ae-446e-9a04-a011fdd49b9e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.1\n"]}]},{"cell_type":"code","source":["import os\n","import csv\n","import json\n","from io import StringIO\n","\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForCausalLM\n","\n","import torch\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, TensorDataset, random_split"],"metadata":{"id":"oU7Al2rC9K3a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["GOOGLE_DRIVE_DIR = \"/content/drive/MyDrive/\"\n","TOKEN_DIR_NAME = \"my_tg_token\"\n","MODEL_DIR_NAME = \"my_tg_model\"\n","\n","DEFAULT_TOKEN = \"tinkoff-ai/ruDialoGPT-medium\"\n","DEFAULT_MODEL = \"tinkoff-ai/ruDialoGPT-medium\"\n","\n","DATA_FILE_NAME = \"data.csv\""],"metadata":{"id":"9NIXRUFnhjb9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_last_directory(base_dir):\n","\n","    base_path = GOOGLE_DRIVE_DIR\n","    max_epoch = -1\n","    max_epoch_path = \"\"\n","\n","    for item in os.listdir(base_path):\n","        if item.startswith(f\"{base_dir}_\"):\n","            try:\n","                epoch = int(item.split('_')[-1])\n","                if epoch > max_epoch:\n","                    max_epoch = epoch\n","                    max_epoch_path = os.path.join(base_path, item)\n","            except ValueError:\n","                continue\n","\n","    if max_epoch != -1:\n","        return max_epoch_path, max_epoch\n","    else:\n","        return None"],"metadata":{"id":"ipxoiUoJY1Rr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files, drive\n","drive.mount('/content/drive')"],"metadata":{"id":"1ueKEsR48cNA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1693472230598,"user_tz":-180,"elapsed":18588,"user":{"displayName":"Iklmn Eprst","userId":"08165371004552365774"}},"outputId":"2efe82f6-962f-4e34-abf4-15894a408403"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["data_file_path = GOOGLE_DRIVE_DIR + DATA_FILE_NAME\n","\n","if not os.path.isfile(data_file_path):\n","    raise Exception(f\"Файл {data_file_path} не найден. Выполнение прервано.\")"],"metadata":{"id":"eBd2XgqjdppS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["token_dir, last_token = find_last_directory(TOKEN_DIR_NAME)\n","\n","if not token_dir:\n","    token_dir = DEFAULT_TOKEN\n","    last_token = 0\n","\n","tokenizer = AutoTokenizer.from_pretrained(token_dir)\n","\n","print(f\"Токенайзер загружен из {token_dir}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2x2JcZ58ZJHR","executionInfo":{"status":"ok","timestamp":1693472242069,"user_tz":-180,"elapsed":1622,"user":{"displayName":"Iklmn Eprst","userId":"08165371004552365774"}},"outputId":"3ddb61da-c789-4b00-e36f-1c329a00456a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Токенайзер загружен из /content/drive/MyDrive/my_tg_token_0\n"]}]},{"cell_type":"code","source":["model_dir, last_epoch = find_last_directory(MODEL_DIR_NAME)\n","\n","if not model_dir:\n","    model_dir = DEFAULT_MODEL\n","    last_epoch = 0\n","\n","model = AutoModelForCausalLM.from_pretrained(model_dir)\n","\n","print(f\"Модель загружена из {model_dir}\")"],"metadata":{"id":"Nlvr0gYdVBzX","executionInfo":{"status":"ok","timestamp":1693472269353,"user_tz":-180,"elapsed":24790,"user":{"displayName":"Iklmn Eprst","userId":"08165371004552365774"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d388a633-ebfa-4224-e644-94426a57964b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Модель загружена из /content/drive/MyDrive/my_tg_model_0\n"]}]},{"cell_type":"code","source":["with open(data_file_path, 'r', newline='', encoding='utf-8') as file:\n","    csv_data = file.read()\n","\n","def csv_to_sequences(csv_str):\n","    sequences = []\n","    csv_reader = csv.reader(StringIO(csv_str), delimiter=',', quotechar='\"')\n","    next(csv_reader)\n","    for row in csv_reader:\n","        sequence_parts = [f\"@@{i.upper()}@@\" + cell for i, cell in zip(['ПЕРВЫЙ', 'ВТОРОЙ'], row) if cell and cell != \"None\"]\n","        sequence = ' '.join(sequence_parts)\n","        sequences.append(sequence)\n","    return sequences\n","\n","\n","sequences = csv_to_sequences(csv_data)\n","\n","print(f\"Общее количество последовательностей в файле данных: {len(sequences)}\")"],"metadata":{"id":"V6VYX855TeO3","executionInfo":{"status":"ok","timestamp":1693472276167,"user_tz":-180,"elapsed":3680,"user":{"displayName":"Iklmn Eprst","userId":"08165371004552365774"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c0343748-1b9d-47f5-86c1-1d4b990291e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Общее количество последовательностей в файле данных: 434488\n"]}]},{"cell_type":"code","source":["seq_to_train = 60000\n","\n","if len(sequences) > seq_to_train:\n","    sequences = sequences[-60000:]\n","    print(f\"Оставляем только последние {seq_to_train} записей\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cRSttQvlpHb0","executionInfo":{"status":"ok","timestamp":1693472281795,"user_tz":-180,"elapsed":251,"user":{"displayName":"Iklmn Eprst","userId":"08165371004552365774"}},"outputId":"c3b263e1-ffc9-4f3e-ed63-dfad04665568"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Оставляем только последние 60000 записей\n"]}]},{"cell_type":"code","source":["tokenized_data = tokenizer(sequences, truncation=True, padding='max_length', max_length=300, return_tensors=\"pt\")\n","\n","last_token += 1\n","token_save_dir = f\"/content/{TOKEN_DIR_NAME}_{last_token}/\"\n","\n","tokenizer.save_pretrained(token_save_dir)\n","print(f\"Токенайзер сохранен в {token_save_dir}\")\n","\n","!cp -r {token_save_dir} {GOOGLE_DRIVE_DIR}\n","print(f\"Копия токенайзера сохранена в {GOOGLE_DRIVE_DIR}\")"],"metadata":{"id":"Dav1FccvU7vO","executionInfo":{"status":"ok","timestamp":1693472302695,"user_tz":-180,"elapsed":14872,"user":{"displayName":"Iklmn Eprst","userId":"08165371004552365774"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"780283c9-ed45-42f3-8ccf-8e55d60a54b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Токенайзер сохранен в /content/my_tg_token_1/\n","Копия токенайзера сохранена в /content/drive/MyDrive/\n"]}]},{"cell_type":"code","source":["dataset = TensorDataset(tokenized_data[\"input_ids\"], tokenized_data[\"attention_mask\"])\n","train_size = int(0.8 * len(dataset))\n","val_size = len(dataset) - train_size\n","train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)"],"metadata":{"id":"kNtErUTaWqho"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Замораживаем все параметры модели\n","for param in model.parameters():\n","    param.requires_grad = False\n","\n","# Размораживаем параметры головы модели\n","for param in model.lm_head.parameters():\n","    param.requires_grad = True"],"metadata":{"id":"5T5zYADvmO-9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.optim.lr_scheduler import ReduceLROnPlateau\n","\n","optimizer = optim.AdamW(model.lm_head.parameters(), lr=5e-5)\n","scheduler = ReduceLROnPlateau(optimizer, 'min')"],"metadata":{"id":"bmZIImBmmWzT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vQf2GWjMnP42","executionInfo":{"status":"ok","timestamp":1693472701322,"user_tz":-180,"elapsed":345,"user":{"displayName":"Iklmn Eprst","userId":"08165371004552365774"}},"outputId":"db7e0aa7-9279-4dc9-ed86-76752dfceb33"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["GPT2LMHeadModel(\n","  (transformer): GPT2Model(\n","    (wte): Embedding(50261, 1024)\n","    (wpe): Embedding(2048, 1024)\n","    (drop): Dropout(p=0.1, inplace=False)\n","    (h): ModuleList(\n","      (0-23): 24 x GPT2Block(\n","        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        (attn): GPT2Attention(\n","          (c_attn): Conv1D()\n","          (c_proj): Conv1D()\n","          (attn_dropout): Dropout(p=0.1, inplace=False)\n","          (resid_dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","        (mlp): GPT2MLP(\n","          (c_fc): Conv1D()\n","          (c_proj): Conv1D()\n","          (act): NewGELUActivation()\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","      )\n","    )\n","    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (lm_head): Linear(in_features=1024, out_features=50261, bias=False)\n",")"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["last_epoch += 1\n","num_epochs = 5\n","\n","steps_output = 50\n","\n","train_losses = []\n","val_losses = []\n","\n","model.train()\n","for epoch in range(last_epoch, last_epoch + num_epochs):\n","    # Тренировочный цикл\n","    total_train_loss = 0\n","    for step, batch in enumerate(train_dataloader):\n","        inputs, masks = batch\n","        inputs = inputs.to(device)\n","        masks = masks.to(device)\n","        outputs = model(inputs, attention_mask=masks, labels=inputs)\n","        loss = outputs.loss\n","        total_train_loss += loss.item()\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        if step != 0 and step % steps_output == 0:\n","            avg_train_loss = total_train_loss / steps_output\n","            print(f\"Train - Epoch: {epoch}, Step: {step}, Loss: {avg_train_loss}\")\n","            train_losses.append(avg_train_loss)\n","            total_train_loss = 0\n","\n","    # Валидационный цикл\n","    model.eval()\n","    total_val_loss = 0\n","    with torch.no_grad():\n","        for batch in val_dataloader:\n","            inputs, masks = batch\n","            inputs = inputs.to(device)\n","            masks = masks.to(device)\n","            outputs = model(inputs, attention_mask=masks, labels=inputs)\n","            total_val_loss += outputs.loss.item()\n","\n","    avg_val_loss = total_val_loss / len(val_dataloader)\n","    print(f\"Validation - Epoch: {epoch}, Avg Loss: {avg_val_loss}\")\n","\n","    val_losses.append(avg_val_loss)\n","\n","    model.save_pretrained(f\"/content/my_tg_model_{epoch}\")\n","    !cp -r /content/my_tg_model_{epoch}/ /content/drive/MyDrive/\n","\n","    with open(f'/content/losses_{epoch}.json', 'w') as f:\n","        json.dump({\"train_losses\": train_losses, \"val_losses\": val_losses}, f)\n","    !cp -r /content/losses_{epoch}.json /content/drive/MyDrive/losses_{epoch}.json\n","\n","    model.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1S5ijPiKnWW2","outputId":"707d7b12-54d9-4ed5-d7bf-c13cd0320a89"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train - Epoch: 1, Step: 50, Loss: 0.27341154105961324\n","Train - Epoch: 1, Step: 100, Loss: 0.2821882022870705\n","Train - Epoch: 1, Step: 150, Loss: 0.2840199191123247\n","Train - Epoch: 1, Step: 200, Loss: 0.27065788842737676\n","Train - Epoch: 1, Step: 250, Loss: 0.24455145407474901\n","Train - Epoch: 1, Step: 300, Loss: 0.26568292509764435\n","Train - Epoch: 1, Step: 350, Loss: 0.260847893844948\n","Train - Epoch: 1, Step: 400, Loss: 0.2758875744492616\n","Train - Epoch: 1, Step: 450, Loss: 0.27446812161009576\n","Train - Epoch: 1, Step: 500, Loss: 0.2856096728917328\n","Train - Epoch: 1, Step: 550, Loss: 0.2615465265880064\n","Train - Epoch: 1, Step: 600, Loss: 0.2608611107204433\n","Train - Epoch: 1, Step: 650, Loss: 0.2999555806070566\n","Train - Epoch: 1, Step: 700, Loss: 0.2479170253324446\n","Train - Epoch: 1, Step: 750, Loss: 0.27138410177081823\n","Train - Epoch: 1, Step: 800, Loss: 0.3085041203722358\n","Train - Epoch: 1, Step: 850, Loss: 0.24847603791467918\n","Train - Epoch: 1, Step: 900, Loss: 0.23770365219563247\n","Train - Epoch: 1, Step: 950, Loss: 0.2699738342347155\n","Train - Epoch: 1, Step: 1000, Loss: 0.2823083494603634\n","Train - Epoch: 1, Step: 1050, Loss: 0.25154454924974173\n","Train - Epoch: 1, Step: 1100, Loss: 0.27912954872857315\n","Train - Epoch: 1, Step: 1150, Loss: 0.2469430059605395\n","Train - Epoch: 1, Step: 1200, Loss: 0.26221820056438444\n","Train - Epoch: 1, Step: 1250, Loss: 0.2810487362459552\n","Train - Epoch: 1, Step: 1300, Loss: 0.2155910252034664\n","Train - Epoch: 1, Step: 1350, Loss: 0.28501040626317264\n","Train - Epoch: 1, Step: 1400, Loss: 0.253550877294349\n","Train - Epoch: 1, Step: 1450, Loss: 0.2514360169693828\n","Train - Epoch: 1, Step: 1500, Loss: 0.2464087815478059\n","Train - Epoch: 1, Step: 1550, Loss: 0.2761474179615418\n","Train - Epoch: 1, Step: 1600, Loss: 0.23464280152121816\n","Train - Epoch: 1, Step: 1650, Loss: 0.30214870497584345\n","Train - Epoch: 1, Step: 1700, Loss: 0.2381128290446346\n","Train - Epoch: 1, Step: 1750, Loss: 0.2668971875299303\n","Train - Epoch: 1, Step: 1800, Loss: 0.22305386315431266\n","Train - Epoch: 1, Step: 1850, Loss: 0.28517307464923775\n","Train - Epoch: 1, Step: 1900, Loss: 0.24231659705373737\n","Train - Epoch: 1, Step: 1950, Loss: 0.31245306001506007\n","Train - Epoch: 1, Step: 2000, Loss: 0.265841718763113\n","Train - Epoch: 1, Step: 2050, Loss: 0.26367345484836785\n","Train - Epoch: 1, Step: 2100, Loss: 0.26578808365389706\n","Train - Epoch: 1, Step: 2150, Loss: 0.2672460079686816\n","Train - Epoch: 1, Step: 2200, Loss: 0.2928818131238222\n","Train - Epoch: 1, Step: 2250, Loss: 0.25572993750044176\n","Train - Epoch: 1, Step: 2300, Loss: 0.23302253905683756\n","Train - Epoch: 1, Step: 2350, Loss: 0.26648961280861455\n","Train - Epoch: 1, Step: 2400, Loss: 0.23993807473808595\n","Train - Epoch: 1, Step: 2450, Loss: 0.23755985779214292\n","Train - Epoch: 1, Step: 2500, Loss: 0.2731319965711206\n","Train - Epoch: 1, Step: 2550, Loss: 0.25937075410038235\n","Train - Epoch: 1, Step: 2600, Loss: 0.26470502541122376\n","Train - Epoch: 1, Step: 2650, Loss: 0.2675823605784452\n","Train - Epoch: 1, Step: 2700, Loss: 0.2533359275758266\n","Train - Epoch: 1, Step: 2750, Loss: 0.25545257721096276\n","Train - Epoch: 1, Step: 2800, Loss: 0.2115192396415511\n","Train - Epoch: 1, Step: 2850, Loss: 0.21711554616689682\n","Train - Epoch: 1, Step: 2900, Loss: 0.24795650333166122\n","Train - Epoch: 1, Step: 2950, Loss: 0.26458706630393863\n","Train - Epoch: 1, Step: 3000, Loss: 0.29562085554900774\n","Train - Epoch: 1, Step: 3050, Loss: 0.24162866773083805\n","Train - Epoch: 1, Step: 3100, Loss: 0.27628093523038766\n","Train - Epoch: 1, Step: 3150, Loss: 0.23751722462674646\n","Train - Epoch: 1, Step: 3200, Loss: 0.22965609349908436\n","Train - Epoch: 1, Step: 3250, Loss: 0.27283125177025797\n","Train - Epoch: 1, Step: 3300, Loss: 0.28504102759187805\n","Train - Epoch: 1, Step: 3350, Loss: 0.21813916074112058\n","Train - Epoch: 1, Step: 3400, Loss: 0.22821305325254798\n","Train - Epoch: 1, Step: 3450, Loss: 0.23684400469064712\n","Train - Epoch: 1, Step: 3500, Loss: 0.27283667578034737\n","Train - Epoch: 1, Step: 3550, Loss: 0.22156171502673175\n","Train - Epoch: 1, Step: 3600, Loss: 0.22852532170812623\n","Train - Epoch: 1, Step: 3650, Loss: 0.2914294264858034\n","Train - Epoch: 1, Step: 3700, Loss: 0.2766677709708744\n","Train - Epoch: 1, Step: 3750, Loss: 0.22750152977712973\n","Train - Epoch: 1, Step: 3800, Loss: 0.24153732433915137\n","Train - Epoch: 1, Step: 3850, Loss: 0.22723796598613263\n","Train - Epoch: 1, Step: 3900, Loss: 0.29360506172288814\n","Train - Epoch: 1, Step: 3950, Loss: 0.2737278520345788\n","Train - Epoch: 1, Step: 4000, Loss: 0.26323237285900375\n","Train - Epoch: 1, Step: 4050, Loss: 0.24180943842977284\n","Train - Epoch: 1, Step: 4100, Loss: 0.25871232595294713\n","Train - Epoch: 1, Step: 4150, Loss: 0.26855194320902226\n","Train - Epoch: 1, Step: 4200, Loss: 0.24019974592665677\n","Train - Epoch: 1, Step: 4250, Loss: 0.27315980026928277\n","Train - Epoch: 1, Step: 4300, Loss: 0.25507553070982963\n","Train - Epoch: 1, Step: 4350, Loss: 0.27503935719805966\n","Train - Epoch: 1, Step: 4400, Loss: 0.25373926514379574\n","Train - Epoch: 1, Step: 4450, Loss: 0.25834195971488955\n","Train - Epoch: 1, Step: 4500, Loss: 0.2141291943192482\n","Train - Epoch: 1, Step: 4550, Loss: 0.25202657394111155\n","Train - Epoch: 1, Step: 4600, Loss: 0.21946063606560295\n","Train - Epoch: 1, Step: 4650, Loss: 0.25722009271383284\n","Train - Epoch: 1, Step: 4700, Loss: 0.26155564964914\n","Train - Epoch: 1, Step: 4750, Loss: 0.26020116472616794\n","Train - Epoch: 1, Step: 4800, Loss: 0.2370640853988661\n","Train - Epoch: 1, Step: 4850, Loss: 0.22326951619100605\n","Train - Epoch: 1, Step: 4900, Loss: 0.2698120512749787\n","Train - Epoch: 1, Step: 4950, Loss: 0.25108304250985386\n","Train - Epoch: 1, Step: 5000, Loss: 0.24757427752017974\n","Train - Epoch: 1, Step: 5050, Loss: 0.26200147961693054\n","Train - Epoch: 1, Step: 5100, Loss: 0.24066934272646903\n","Train - Epoch: 1, Step: 5150, Loss: 0.2602204953879118\n","Train - Epoch: 1, Step: 5200, Loss: 0.27707038149237634\n","Train - Epoch: 1, Step: 5250, Loss: 0.26471024979759317\n","Train - Epoch: 1, Step: 5300, Loss: 0.19627408528164722\n","Train - Epoch: 1, Step: 5350, Loss: 0.23708913549780847\n","Train - Epoch: 1, Step: 5400, Loss: 0.24334569185972213\n","Train - Epoch: 1, Step: 5450, Loss: 0.2517203574627638\n","Train - Epoch: 1, Step: 5500, Loss: 0.24370940543711186\n","Train - Epoch: 1, Step: 5550, Loss: 0.2618068121052159\n","Train - Epoch: 1, Step: 5600, Loss: 0.251624232345567\n","Train - Epoch: 1, Step: 5650, Loss: 0.27381381295621393\n","Train - Epoch: 1, Step: 5700, Loss: 0.27205973374657333\n","Train - Epoch: 1, Step: 5750, Loss: 0.17763080194250414\n","Train - Epoch: 1, Step: 5800, Loss: 0.24682777717709542\n","Train - Epoch: 1, Step: 5850, Loss: 0.2574677271768451\n","Train - Epoch: 1, Step: 5900, Loss: 0.20136364135290732\n","Train - Epoch: 1, Step: 5950, Loss: 0.23481774754822254\n","Validation - Epoch: 1, Avg Loss: 0.24755955895905693\n","Train - Epoch: 2, Step: 50, Loss: 0.23515400443425447\n","Train - Epoch: 2, Step: 100, Loss: 0.2622459312528372\n","Train - Epoch: 2, Step: 150, Loss: 0.2288543692846691\n","Train - Epoch: 2, Step: 200, Loss: 0.23283027589321137\n","Train - Epoch: 2, Step: 250, Loss: 0.25001350827515123\n","Train - Epoch: 2, Step: 300, Loss: 0.24534525961269363\n","Train - Epoch: 2, Step: 350, Loss: 0.2414236812363301\n","Train - Epoch: 2, Step: 400, Loss: 0.24160623716190457\n","Train - Epoch: 2, Step: 450, Loss: 0.22981386162340642\n","Train - Epoch: 2, Step: 500, Loss: 0.23556582244290297\n","Train - Epoch: 2, Step: 550, Loss: 0.22761465173214673\n","Train - Epoch: 2, Step: 600, Loss: 0.23097164859063923\n","Train - Epoch: 2, Step: 650, Loss: 0.2540555656887591\n","Train - Epoch: 2, Step: 700, Loss: 0.2490542079694569\n","Train - Epoch: 2, Step: 750, Loss: 0.2542526364326477\n","Train - Epoch: 2, Step: 800, Loss: 0.23761666961014272\n","Train - Epoch: 2, Step: 850, Loss: 0.1939929484948516\n","Train - Epoch: 2, Step: 900, Loss: 0.2236319632248883\n","Train - Epoch: 2, Step: 950, Loss: 0.2032122848252688\n","Train - Epoch: 2, Step: 1000, Loss: 0.24301600232720375\n","Train - Epoch: 2, Step: 1050, Loss: 0.2433351958769196\n","Train - Epoch: 2, Step: 1100, Loss: 0.21318359976634382\n","Train - Epoch: 2, Step: 1150, Loss: 0.23207535017281772\n","Train - Epoch: 2, Step: 1200, Loss: 0.2118178711750079\n","Train - Epoch: 2, Step: 1250, Loss: 0.23852603051811458\n","Train - Epoch: 2, Step: 1300, Loss: 0.23703043684363365\n","Train - Epoch: 2, Step: 1350, Loss: 0.21258502550423145\n","Train - Epoch: 2, Step: 1400, Loss: 0.22592928810045124\n","Train - Epoch: 2, Step: 1450, Loss: 0.2524317853152752\n","Train - Epoch: 2, Step: 1500, Loss: 0.22857855260471838\n","Train - Epoch: 2, Step: 1550, Loss: 0.2800204138597318\n","Train - Epoch: 2, Step: 1600, Loss: 0.25725195605831075\n","Train - Epoch: 2, Step: 1650, Loss: 0.19571063919092552\n","Train - Epoch: 2, Step: 1700, Loss: 0.23020233609641966\n","Train - Epoch: 2, Step: 1750, Loss: 0.26926113948225977\n","Train - Epoch: 2, Step: 1800, Loss: 0.20006805593446894\n","Train - Epoch: 2, Step: 1850, Loss: 0.21869714120434078\n","Train - Epoch: 2, Step: 1900, Loss: 0.22492564093321563\n","Train - Epoch: 2, Step: 1950, Loss: 0.19858438484370708\n","Train - Epoch: 2, Step: 2000, Loss: 0.2304479422606528\n","Train - Epoch: 2, Step: 2050, Loss: 0.2701681434731293\n","Train - Epoch: 2, Step: 2100, Loss: 0.2437605644389987\n","Train - Epoch: 2, Step: 2150, Loss: 0.23092970082467812\n","Train - Epoch: 2, Step: 2200, Loss: 0.2531131130829454\n","Train - Epoch: 2, Step: 2250, Loss: 0.28053835928440096\n","Train - Epoch: 2, Step: 2300, Loss: 0.2495789268702788\n","Train - Epoch: 2, Step: 2350, Loss: 0.19633747454732656\n","Train - Epoch: 2, Step: 2400, Loss: 0.21266468809917569\n","Train - Epoch: 2, Step: 2450, Loss: 0.2632628729194403\n","Train - Epoch: 2, Step: 2500, Loss: 0.21329759258776904\n","Train - Epoch: 2, Step: 2550, Loss: 0.24540437761694192\n","Train - Epoch: 2, Step: 2600, Loss: 0.23381946599939493\n","Train - Epoch: 2, Step: 2650, Loss: 0.21465968802571297\n","Train - Epoch: 2, Step: 2700, Loss: 0.2568111541887191\n","Train - Epoch: 2, Step: 2750, Loss: 0.2628324272483587\n","Train - Epoch: 2, Step: 2800, Loss: 0.21110865148948504\n","Train - Epoch: 2, Step: 2850, Loss: 0.2543200866355712\n","Train - Epoch: 2, Step: 2900, Loss: 0.21794771917164327\n","Train - Epoch: 2, Step: 2950, Loss: 0.2219380483513538\n","Train - Epoch: 2, Step: 3000, Loss: 0.22832648297771813\n","Train - Epoch: 2, Step: 3050, Loss: 0.23258155543357134\n","Train - Epoch: 2, Step: 3100, Loss: 0.28578945584595206\n","Train - Epoch: 2, Step: 3150, Loss: 0.26750613144060026\n"]}]}]}